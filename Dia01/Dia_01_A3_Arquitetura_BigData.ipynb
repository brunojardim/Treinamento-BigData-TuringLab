{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Arquitetura que vamos trabalhar neste treinamento"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/html": ["<img src=\"https://s3.amazonaws.com/treinamento-big-data/imagens/arq_treinamento_01.PNG\" width=\"1000\" height=\"1000\"/>"], "text/plain": ["<IPython.core.display.Image object>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["from IPython.display import Image\n", "from IPython.core.display import HTML \n", "Image(url= \"https://s3.amazonaws.com/treinamento-big-data/imagens/arq_treinamento_01.PNG\", width=1000, height=1000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Resumo sobre alguns aplicativos"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["### 1) ZooKeeper: ZooKeeper \u00e9 um servi\u00e7o de coordena\u00e7\u00e3o distribu\u00edda para gerenciar grandes conjuntos de Clusters.\n", "\n", "### 2) Oozie: Apache Oozie \u00e9 um sistema de agendamento de WorkFlow, usado para gerenciar principalmente os Jobs de MapReduce.\n", "\n", "### 3) Pig: Apache Pig, \u00e9 uma linguagem de procedimentos de alto n\u00edvel para consultar grandes conjuntos de dados semiestruturados usando Hadoop e a Plataforma MapReduce\n", "\n", "### 4) Sqoop: Apache Sqoop, \u00e9 um projeto do ecossistema Hadoop, cuja responsabilidade \u00e9 importar e exportar dados do banco de dados de dados relacionais.\n", "\n", "### 5) Spark: Apache Spark, \u00e9 uma ferramenta Big Data para o processamento de grandes conjuntos de dados. Foi desenvolvido para substituir o MapReduce, pois processa 100x mais r\u00e1pido que o MapReduce.\n", "\n", "### 6) HBase: Apache Hbase, \u00e9 um banco de Dados n\u00e3o relacionais, projetado para trabalhar com grande conjunto de dados (Big Data). \u00c9 o banco de dados oficial do hadoop.\n", "\n", "### 7) Flume: Apache Flume, \u00e9 um servi\u00e7o que permite enviar dados diretamente para o HDFS. \u00c9 um servi\u00e7o que funciona em ambiente dstribuido (em cluster) para coletar, agregar e mover grandes quantidades de dados de forma eficiente.\n", "\n", "### 8) Mahout: Apache Mahout, \u00e9 dedicado a Machine Learning \u2013 Data Science. Ele permite a utiliza\u00e7\u00e3o dos principais algoritmos de clustering, testes de regress\u00e3o e modelagem estat\u00edstica.\n", "\n", "### 9) Kafka: Apache Kafka, \u00e9 foi desenvolvido pelo Linkedin e liberado como projeto Open-source em 2011. O Apache Kafka \u00e9 um sistema para gerenciamento de fluxo de dados em tempo real, gerados a partir de websites, aplica\u00e7\u00f5es e sensores.\n", "\n", "### 10) Ambari: Apache Ambari tem como objetivo tornar o gerenciamento do Hadoop mais simples. O Ambari fornece uma interface de usu\u00e1rio da Web de gerenciamento do Hadoop intuitiva e f\u00e1cil de usar.\n", "\n", "### 11) Livy: Permite o envio program\u00e1tico, tolerante a falhas e de v\u00e1rios usu\u00e1rios de tarefas do Spark a partir de aplicativos da Web / m\u00f3veis (n\u00e3o \u00e9 necess\u00e1rio cliente do Spark). Assim, v\u00e1rios usu\u00e1rios podem interagir com seu cluster Spark simultaneamente e de forma confi\u00e1vel\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}}, "nbformat": 4, "nbformat_minor": 2}